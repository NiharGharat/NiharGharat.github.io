{
    "companyName" : "LTI",
    "identifier" : "lti",
    "baseLocation" : "Pune",
    "country" : "India",
    "skills" : [
        "Java", "SpringBoot", "Docker", "Kubernetes", "MySQL", "PostgreSQL", "Snowflake", "Hive", "MongoDB", "Azure Blob", "S3", "GCP", "Spark", "jvisualVM", "bash", "Git", "Sublime3"
    ],
    "why": "Learnt everything, coding from scratch to Ubuntu",
    "duration": "2018-2021",
    "companyLoved4Points" : [
        "Part of product engineering team, working on the connector framework - ie - reading data from any source(RDBMS, Cloud, etc.), and pushing to any source",
        "Developed upsert implementation from scratch for mongoDB on Spark dataset API",
        "Supported implementation of 15 connectors ranging from AzureBlob, S3, file formats like json, csv, parquet, xml, etc., RDBMS like snowflake, MySQL, postgreSQL, etc.",
        "Loved working on core functionality, thinking about problems at scale"
    ],
    "order": 2,
    "companyDetailPoints" : [
        "Designed and developed REST APIs on Java for services like connector and scanner service; developed data parsers(connectors) for various sources like RDBMS systems (Postgres, MySql, Hive, SqlServer, Snowflake, etc.); file formats like xlsx, delimited, sas7bdat, json, xml; NoSql like mongoDb; and crm platforms like Salesforce.",
        "Worked on cloud connectors like Azure Blob, Amazon S3, GCP; with a major focus on Azure Blob.",
        "Improved performance of Xlsx data parser by ~50%, by identifying bottlenecks in Apache Poi implementation. Designed, implemented, and maintained new features like range read, streaming read, etc. for xlsx connector.",
        "Developed bash scripts on Linux for doing automated performance benchmarking for various connectors for Spark on K8S.",
        "Designed and developed functionality like upsert for RDBMS via spark, mongoDb authentication.",
        "Researched the feasibility of a connector for Google Drive. Developed a POC to demo the integrated functionality.",
        "Worked on other technologies like Docker, Kubernetes, Azure Blob, Amazon S3, Sftp, Spring boot, CI/CD, Snowflake, SonarQube, ElasticSearch. Strong experience in Linux, with Ubuntu as the choice of development OS.",
        "Contributed to maintenance and enhancement of Spark cluster processing for about 15 connectors.",
        "Worked on file connectors like Delimited, Json, Xlsx, Parquet, Xml, Sas7bdat"
    ],
    "highlights" : [
        {
            "type": "perf_improvement",
            "name": "XLSX connector performance improvement",
            "logo": "mock_story_connector_apache_poi.png",
            "stmt": [
                "Apache POI was the best for working on xlsx, but it was the slowest impl when it came to files of size > 50MB. I was heavily involved in identifying the bottlenecks at the worksheet object creation, looking for alternatives, and poc on different libraries usage. Firsthand learning using jvisualVM for analysing JVMs",
                "Integrated a streaming based approach for reading huge xlsx files. Ensured all custom features worked with newer approach."
            ]
        },
        {
            "type": "perf_improvement",
            "name": "JSON connector performance improvement",
            "logo": "mock_story_connector_json.png",
            "stmt": [
                "Involved in refractoring Json reading impl 3 times, with the last refractor was on top of Spark own impl. Sparks impl caused many issues with multiLine. Our impl was lighter, used Gson streaming, worked on spark cluster, reading json as bytes line by line to create a Dataset.",
                "This approach was the best, as it worked on multi-line json, jsonL, and beautified json perfectly"
            ]
        },
        {
            "type": "extra",
            "name": "Git",
            "logo": "mock_story_git.png",
            "stmt": ["One of the maintainer of our repo, solving all git issues. Used a linear history model, which was easier to comprenend"]
        },
        {
            "type": "development",
            "name": "Custom features on XLSX/CSV",
            "logo": "mock_story_connector_apache_poi.png",
            "stmt": ["Implemented all custom features on xlsx, delimited files. Language support and encoding caused some issues when spark/non-spark libraries were used. But, I ensured the impl were as consistent as possible"]
        },
        {
            "type": "development",
            "name": "Refractor for better design",
            "logo": "mock_story_connector_jdbc.png",
            "stmt": ["Designed and implemented better single responsibility for present class structures, while implementing advance connection url for jdbc with all SQL connectors. It allowed better creation of customised connection url string while creation of specific SQL connectors"]
        },
        {
            "type": "extra",
            "name": "Bash Scripts",
            "logo": "mock_story_extra_bash_scripts.png",
            "stmt": [
                "Bash scripting is my favourite. I started to write bash scripts for almost all mundane jobs. Honorary mention - while doing performance benchmarking, I developed a script that will spawn pods(Kubernetes) of different configs, for specified connector type, and sleep in background. It will periodically check the pods completion status, and once done, it will grep the log, and prune the the timings to present the result. EOD - the performance timings were ready for me in a csv. All this thanks to the support team not tightly controlling the environment",
                "Wrote another bash script for pushing our latest code on supports git env for deployment only based on image tags. The script would \"correctly\" rebase main, and push our new tags on the deploy file, and push with a consistent commit message. Also, handled scenarios were there would be conflicts on pull/push. With the script, you just need to have the image tags ready, dont need to interact with git"
                    ]
        }
    ],
    "logo": "lti.png"
} 